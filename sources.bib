

@misc{zhang_evaluating_2025,
	title = {Evaluating Trust in {AI}, Human, and Co-produced Feedback Among Undergraduate Students},
	url = {http://arxiv.org/abs/2504.10961},
	doi = {10.48550/arXiv.2504.10961},
	abstract = {As generative {AI} transforms educational feedback practices, understanding students' perceptions of different feedback providers becomes crucial for effective implementation. This study addresses a critical gap by comparing undergraduate students' trust in {AI}-generated, human-created, and human-{AI} co-produced feedback, informing how institutions can adapt feedback practices in this new era. Through a within-subject experiment with 91 participants, we investigated factors predicting students' ability to distinguish between feedback types, perception of feedback quality, and potential biases to {AI} involvement. Findings revealed that students generally preferred {AI} and co-produced feedback over human feedback in terms of perceived usefulness and objectivity. Only {AI} feedback suffered a decline in perceived genuineness when feedback sources were revealed, while co-produced feedback maintained its positive perception. Educational {AI} experience improved students' ability to identify {AI} feedback and increased their trust in all feedback types, while general {AI} experience decreased perceived usefulness and credibility. Male students consistently rated all feedback types as less valuable than their female and non-binary counterparts. These insights inform evidence-based guidelines for integrating {AI} into higher education feedback systems while addressing trust concerns and fostering {AI} literacy among students.},
	number = {{arXiv}:2504.10961},
	publisher = {{arXiv}},
	author = {Zhang, Audrey and Gao, Yifei and Suraworachet, Wannapon and Nazaretsky, Tanya and Cukurova, Mutlu},
	urldate = {2025-05-13},
	date = {2025-04-15},
	eprinttype = {arxiv},
	eprint = {2504.10961 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:/Users/11nho/Zotero/storage/PVWMQUGV/Zhang et al. - 2025 - Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students.pdf:application/pdf;Snapshot:/Users/11nho/Zotero/storage/47JRDT8E/2504.html:text/html},
}

@misc{brauner_mapping_2024,
	title = {Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance},
	url = {http://arxiv.org/abs/2411.19356},
	doi = {10.48550/arXiv.2411.19356},
	shorttitle = {Mapping Public Perception of Artificial Intelligence},
	abstract = {Understanding public perception of artificial intelligence ({AI}) and the tradeoffs between potential risks and benefits is crucial, as these perceptions might shape policy decisions, influence innovation trajectories for successful market strategies, and determine individual and societal acceptance of {AI} technologies. Using a representative sample of 1100 participants from Germany, this study examines mental models of {AI}. Participants quantitatively evaluated 71 statements about {AI}'s future capabilities (e.g., autonomous driving, medical care, art, politics, warfare, and societal divides), assessing the expected likelihood of occurrence, perceived risks, benefits, and overall value. We present rankings of these projections alongside visual mappings illustrating public risk-benefit tradeoffs. While many scenarios were deemed likely, participants often associated them with high risks, limited benefits, and low overall value. Across all scenarios, 96.4\% (\$r{\textasciicircum}2=96.4{\textbackslash}\%\$) of the variance in value assessment can be explained by perceived risks (\${\textbackslash}beta=-.504\$) and perceived benefits (\${\textbackslash}beta=+.710\$), with no significant relation to expected likelihood. Demographics and personality traits influenced perceptions of risks, benefits, and overall evaluations, underscoring the importance of increasing {AI} literacy and tailoring public information to diverse user needs. These findings provide actionable insights for researchers, developers, and policymakers by highlighting critical public concerns and individual factors essential to align {AI} development with individual values.},
	number = {{arXiv}:2411.19356},
	publisher = {{arXiv}},
	author = {Brauner, Philipp and Glawe, Felix and Liehner, Gian Luca and Vervier, Luisa and Ziefle, Martina},
	urldate = {2025-05-13},
	date = {2024-11-28},
	eprinttype = {arxiv},
	eprint = {2411.19356 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:/Users/11nho/Zotero/storage/AXCCGJ2Q/Brauner et al. - 2024 - Mapping Public Perception of Artificial Intelligence Expectations, Risk-Benefit Tradeoffs, and Valu.pdf:application/pdf;Snapshot:/Users/11nho/Zotero/storage/L6A7V23H/2411.html:text/html},
}

@article{araujo_ai_2020,
	title = {In {AI} we trust? Perceptions about automated decision-making by artificial intelligence},
	volume = {35},
	issn = {0951-5666, 1435-5655},
	url = {http://link.springer.com/10.1007/s00146-019-00931-w},
	doi = {10.1007/s00146-019-00931-w},
	shorttitle = {In {AI} we trust?},
	abstract = {Fueled by ever-growing amounts of (digital) data and advances in artificial intelligence, decision-making in contemporary societies is increasingly delegated to automated processes. Drawing from social science theories and from the emerging body of research about algorithmic appreciation and algorithmic perceptions, the current study explores the extent to which personal characteristics can be linked to perceptions of automated decision-making by {AI}, and the boundary conditions of these perceptions, namely the extent to which such perceptions differ across media, (public) health, and judicial contexts. Data from a scenario-based survey experiment with a national sample (N = 958) show that people are by and large concerned about risks and have mixed opinions about fairness and usefulness of automated decision-making at a societal level, with general attitudes influenced by individual characteristics. Interestingly, decisions taken automatically by {AI} were often evaluated on par or even better than human experts for specific decisions. Theoretical and societal implications about these findings are discussed.},
	pages = {611--623},
	number = {3},
	journaltitle = {{AI} \& {SOCIETY}},
	shortjournal = {{AI} \& Soc},
	author = {Araujo, Theo and Helberger, Natali and Kruikemeier, Sanne and De Vreese, Claes H.},
	urldate = {2025-05-13},
	date = {2020-09},
	langid = {english},
	file = {PDF:/Users/11nho/Zotero/storage/ZXJL6B4P/Araujo et al. - 2020 - In AI we trust Perceptions about automated decision-making by artificial intelligence.pdf:application/pdf},
}

@misc{fast_long-term_2016,
	title = {Long-Term Trends in the Public Perception of Artificial Intelligence},
	url = {http://arxiv.org/abs/1609.04904},
	doi = {10.48550/arXiv.1609.04904},
	abstract = {Analyses of text corpora over time can reveal trends in beliefs, interest, and sentiment about a topic. We focus on views expressed about artificial intelligence ({AI}) in the New York Times over a 30-year period. General interest, awareness, and discussion about {AI} has waxed and waned since the field was founded in 1956. We present a set of measures that captures levels of engagement, measures of pessimism and optimism, the prevalence of specific hopes and concerns, and topics that are linked to discussions about {AI} over decades. We find that discussion of {AI} has increased sharply since 2009, and that these discussions have been consistently more optimistic than pessimistic. However, when we examine specific concerns, we find that worries of loss of control of {AI}, ethical concerns for {AI}, and the negative impact of {AI} on work have grown in recent years. We also find that hopes for {AI} in healthcare and education have increased over time.},
	number = {{arXiv}:1609.04904},
	publisher = {{arXiv}},
	author = {Fast, Ethan and Horvitz, Eric},
	urldate = {2025-05-13},
	date = {2016-12-02},
	eprinttype = {arxiv},
	eprint = {1609.04904 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Full Text PDF:/Users/11nho/Zotero/storage/MV6WNIAJ/Fast and Horvitz - 2016 - Long-Term Trends in the Public Perception of Artificial Intelligence.pdf:application/pdf;Snapshot:/Users/11nho/Zotero/storage/IWSZJNVP/1609.html:text/html},
}

@article{thatikonda_implications_nodate,
	title = {{IMPLICATIONS} {OF} {AI} {ON} {JOB} {OPPORTUNITIES} {FOR} {ENTRY}-{LEVEL} {SOFTWARE} {DEVELOPERS}},
	abstract = {Artificial Intelligence ({AI}) is a growing technology in the software industry that can mimic human intelligence. Prior research has shown a direct relationship between {AI} and job displacement in the software industry as the technology spreads in the industry. However, it has only explored {AI}’s effects on current employees, which reveals a gap concerning {AI}’s effect on the employment of new developers. To evaluate this issue, the study used standardized qualitative interviews with software employers. The data concluded that {AI} is too primitive to cause job displacement risk, but rather automation programs have been a leading cause of job loss. Future research can explore the specific effects of {AI} on software companies and the causes of their rapid growth in the industry.},
	author = {Thatikonda, Harshith},
	langid = {english},
	file = {PDF:/Users/11nho/Zotero/storage/JT2V6DXK/Thatikonda - IMPLICATIONS OF AI ON JOB OPPORTUNITIES FOR ENTRY-LEVEL SOFTWARE DEVELOPERS.pdf:application/pdf},
}
